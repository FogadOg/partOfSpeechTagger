{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "kuahz_6vBnWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "b6s6Pfue8jVH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import re\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQxLdXNWBkIg",
        "outputId": "3be12f1c-01be-40b6-b330-923e4ff24194"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgFPA1AOBmgW",
        "outputId": "e3b2b1ee-8ffb-42d1-d1a6-a8cee2e003a1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "ZjvmMP6R86ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokinizer"
      ],
      "metadata": {
        "id": "2uV56-tyDpWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokinizer():\n",
        "  def __init__(self, maxSquenceLength=200):\n",
        "    self.maxSquenceLength=maxSquenceLength\n",
        "\n",
        "    self.tokinizerDict=dict({\"PAD\":0, \"UNK\":1})\n",
        "\n",
        "  def encode(self, sentanceList: list[str])->list[int]:\n",
        "    encodedSentance=[]\n",
        "\n",
        "    for word in sentanceList:\n",
        "      if len(encodedSentance)>=self.maxSquenceLength:\n",
        "        break\n",
        "\n",
        "      if word in self.tokinizerDict.keys():\n",
        "        encodedSentance.append(self.tokinizerDict[word])\n",
        "      else:\n",
        "        encodedSentance.append(self.tokinizerDict[\"UNK\"])\n",
        "\n",
        "\n",
        "    return self.addPaddingToEncoding(encodedSentance)\n",
        "\n",
        "\n",
        "  def addPaddingToEncoding(self, encoding: list[int])->list[int]:\n",
        "    paddingLengthRequired=self.maxSquenceLength-len(encoding)\n",
        "    paddingArray=[self.tokinizerDict[\"PAD\"]]*paddingLengthRequired\n",
        "\n",
        "    return encoding+paddingArray\n",
        "\n",
        "\n",
        "  def decode(self, encodedSentance: list[int])->list[str]:\n",
        "    decodedString=\"\"\n",
        "    dictKeys=list(self.tokinizerDict.keys())\n",
        "\n",
        "    for token in encodedSentance:\n",
        "      try:\n",
        "        wordPosition=list(self.tokinizerDict.values()).index(token)\n",
        "        decodedString+=dictKeys[wordPosition]\n",
        "      except ValueError:\n",
        "        raise ValueError(f\"token {token} was not found in dictionary\")\n",
        "\n",
        "    return decodedString\n",
        "\n",
        "\n",
        "  def loadTokinizerDictionary(self, filePath)->None:\n",
        "    jsonfile=open(filePath)\n",
        "    jsonObject=json.load(jsonfile)\n",
        "    self.tokinizerDict=jsonObject\n",
        "\n",
        "\n",
        "  def createDataset(self, dataset: list[str], fileSavePath=None)->None:\n",
        "    for sentance in dataset:\n",
        "      for word in sentance.split(\" \"):\n",
        "\n",
        "        if word in self.tokinizerDict:\n",
        "          pass\n",
        "        else:\n",
        "          self.tokinizerDict[word]=len(self.tokinizerDict)\n",
        "\n",
        "    if fileSavePath !=None:\n",
        "      self._saveDictionary(fileSavePath )\n",
        "\n",
        "\n",
        "  def _saveDictionary(self, savePath):\n",
        "    with open(savePath, \"w\") as outfile:\n",
        "      json.dump(self.tokinizerDict, outfile)"
      ],
      "metadata": {
        "id": "HVckFoZsFEGT"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Cleaner\n"
      ],
      "metadata": {
        "id": "5WINEWZHFElT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def processText(self, string:str)->str:\n",
        "    removePunctuations = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', string)\n",
        "    removeMultipuleSpace = re.sub(r'\\s+', ' ', removePunctuations)\n",
        "\n",
        "    return removeMultipuleSpace\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KFQHON_XFJjU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make Dataset"
      ],
      "metadata": {
        "id": "7k5m088A-KHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PosDataset(Dataset):\n",
        "\n",
        "  def __init__(self,filePath):\n",
        "    self.tokinizer=Tokinizer()\n",
        "\n",
        "    self.tokinizer.createDataset([\"this is some test data to fit the tokinizer\"],\"/content/drive/MyDrive/posTagging/tokinizerDict.json\")\n",
        "    self.tokinizer.loadTokinizerDictionary(\"/content/drive/MyDrive/posTagging/tokinizerDict.json\")\n",
        "\n",
        "    self.filePath=filePath\n",
        "\n",
        "    self.datasetSentances=torch.tensor(self.unpackSentances())\n",
        "    self.datasetLabels=torch.tensor(self.unpackLabels())\n",
        "\n",
        "  def unpackSentances(self)->list[int]:\n",
        "    jsonFile = open(self.filePath)\n",
        "    jsonArray = json.load(jsonFile)\n",
        "\n",
        "    sentanceList=[]\n",
        "\n",
        "    for object in jsonArray:\n",
        "      tokinizedSentace=self.tokinizedSentace(object[\"sentence\"])\n",
        "\n",
        "      sentanceList.append(tokinizedSentace)\n",
        "\n",
        "    return sentanceList\n",
        "\n",
        "  def unpackLabels(self)->list[int]:\n",
        "    jsonFile = open(self.filePath)\n",
        "    jsonArray = json.load(jsonFile)\n",
        "\n",
        "    labelsList=[]\n",
        "\n",
        "    for object in jsonArray:\n",
        "      tokinizedSentace=self.tokinizedSentace(object[\"sentence\"])\n",
        "      tokinizedLabels=self.tokinizedLabels(object[\"labels\"])\n",
        "\n",
        "\n",
        "      labelsList.append(tokinizedLabels)\n",
        "\n",
        "    return labelsList\n",
        "\n",
        "\n",
        "  def tokinizedSentace(self, sentance: list[str])->list[int]:\n",
        "    return self.tokinizer.encode(sentance)\n",
        "\n",
        "  def tokinizedLabels(self, labels: list[str])->list[int]:\n",
        "    return self.tokinizer.encode(labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.datasetSentances[idx],self.datasetLabels[idx]\n",
        "\n",
        "  def __len__(self)->int:\n",
        "    return len(self.datasetSentances);\n",
        "\n",
        "dataset=PosDataset(\"/content/drive/MyDrive/posTagging/test.json\")"
      ],
      "metadata": {
        "id": "pPx4AOu8AllD"
      },
      "execution_count": 103,
      "outputs": []
    }
  ]
}