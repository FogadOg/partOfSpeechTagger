{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\npart of speech model.ipynb<br>\n", "Automatically generated by Colaboratory.<br>\n", "Original file is located at<br>\n", "    https://colab.research.google.com/drive/1wntZTVBpkuhU7hy60nzog1HbKo6U-71Q<br>\n", "#Imports<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import json\n", "import re\n", "from torch import nn\n", "from torch.utils.data import Dataset, DataLoader\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BATCH_SIZE=64"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#Data Preparation<br>\n", "##Tokinizer<br>\n", "###Tokinizer Util<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TokinizerDatasetUtils():\n", "  def __init__(self, maxDictLen=10000):\n", "    self.maxDictLen=maxDictLen\n", "    self.tokinizerDict=dict({\"UNK\":0, \"PAD\":1})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def loadTokinizerDictionary(self, filePath)->None:\n", "    jsonfile=open(filePath)\n", "    jsonObject=json.load(jsonfile)\n", "    self.tokinizerDict=jsonObject"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def createDataset(self, dataset: list[list[str]], fileSavePath=None)->None:\n", "    for sentance in dataset:\n", "      for word in sentance:\n", "        if self.maxDictLen==len(self.tokinizerDict):\n", "          break\n", "        if word not in self.tokinizerDict:\n", "          self.tokinizerDict[word]=len(self.tokinizerDict)\n", "    if fileSavePath !=None:\n", "      self._saveDictionary(fileSavePath)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def extractSentanceText(self, fileSavePath=None, *DatasetfilePaths)->None:\n", "    dataset=self._extractData(\"sentence\", DatasetfilePaths)\n", "    self.createDataset(dataset, fileSavePath)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def extractLabelText(self, fileSavePath=None, *DatasetfilePaths)->None:\n", "    dataset=self._extractData(\"labels\", DatasetfilePaths)\n", "    self.createDataset(dataset, fileSavePath)\n", "    self.createDataset(dataset, fileSavePath)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def _extractData(self, field: str, DatasetfilePaths):\n", "    dataset=[]\n", "    for DatasetfilePath in DatasetfilePaths:\n", "      jsonFile = open(DatasetfilePath)\n", "      jsonArray = json.load(jsonFile)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["      for object in jsonArray:\n", "        dataset.append(object[field])\n", "    return dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def _saveDictionary(self, savePath):\n", "    with open(savePath, \"w\") as outfile:\n", "      json.dump(self.tokinizerDict, outfile)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n###Tokinizer Class\n<br>\n", "class Tokinizer(TokinizerDatasetUtils):<br>\n", "  def __init__(self, maxDictLen=10000, maxSquenceLength=100):<br>\n", "    super().__init__(maxDictLen)<br>\n", "    self.maxSquenceLength=maxSquenceLength<br>\n", "  def encode(self, sentanceList: list[str])->list[int]:<br>\n", "    encodedSentance=[]<br>\n", "    for word in sentanceList:<br>\n", "      if len(encodedSentance)>=self.maxSquenceLength:<br>\n", "        break<br>\n", "      if word in self.tokinizerDict.keys():<br>\n", "        encodedSentance.append(self.tokinizerDict[word])<br>\n", "      else:<br>\n", "        encodedSentance.append(self.tokinizerDict[\"UNK\"])<br>\n", "    return self.addPaddingToEncoding(encodedSentance)<br>\n", "  def addPaddingToEncoding(self, encoding: list[int])->list[int]:<br>\n", "    paddingLengthRequired=self.maxSquenceLength-len(encoding)<br>\n", "    paddingArray=[self.tokinizerDict[\"PAD\"]]*paddingLengthRequired<br>\n", "    return encoding+paddingArray<br>\n", "  def decode(self, encodedSentance: list[int])->list[str]:<br>\n", "    decodedString=\"\"<br>\n", "    dictKeys=list(self.tokinizerDict.keys())<br>\n", "    for token in encodedSentance:<br>\n", "      try:<br>\n", "        wordPosition=list(self.tokinizerDict.values()).index(token)<br>\n", "        decodedString+=dictKeys[wordPosition]<br>\n", "      except ValueError:<br>\n", "        raise ValueError(f\"token {token} was not found in dictionary\")<br>\n", "    return decodedString<br>\n", "  def __len__(self):<br>\n", "    return len(self.tokinizerDict)<br>\n", "#Make Dataset\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PosDataset(Dataset):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def __init__(self,filePaths: list):\n", "    self.tokinizerSentance=Tokinizer(maxDictLen=2000)\n", "    self.tokinizerLabel=Tokinizer(maxDictLen=2000)\n", "    self.tokinizerSentance.extractSentanceText(\"tokinizerSentanceDict.json\", \"train.json\", \"test.json\")\n", "    self.tokinizerSentance.loadTokinizerDictionary(\"tokinizerSentanceDict.json\")\n", "    self.tokinizerLabel.extractSentanceText(\"tokinizerLabelDict.json\", \"train.json\", \"test.json\")\n", "    self.tokinizerLabel.loadTokinizerDictionary(\"tokinizerLabelDict.json\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    self.filePaths=filePaths\n", "    self.datasetSentances=torch.tensor(self.unpackSentances())\n", "    self.datasetLabels=torch.tensor(self.unpackLabels())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def unpackSentances(self)->list[int]:\n", "    sentanceList=[]\n", "    for filePath in self.filePaths:\n", "      jsonFile = open(filePath)\n", "      jsonArray = json.load(jsonFile)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["      for object in jsonArray:\n", "        tokinizedSentace=self.tokinizedSentace(object[\"sentence\"])\n", "        sentanceList.append(tokinizedSentace)\n", "    return sentanceList"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def unpackLabels(self)->list[int]:\n", "    labelsList=[]\n", "    for filePath in self.filePaths:\n", "      jsonFile = open(filePath)\n", "      jsonArray = json.load(jsonFile)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["      for object in jsonArray:\n", "        tokinizedLabels=self.tokinizedLabels(object[\"labels\"])\n", "        labelsList.append(tokinizedLabels)\n", "    return labelsList"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def tokinizedSentace(self, sentance: list[str])->list[int]:\n", "    return self.tokinizerSentance.encode(sentance)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def tokinizedLabels(self, labels: list[str])->list[int]:\n", "    return self.tokinizerLabel.encode(labels)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def __getitem__(self,idx):\n", "    return self.datasetSentances[idx],self.datasetLabels[idx]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def __len__(self)->int:\n", "    return len(self.datasetSentances);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset=PosDataset([\"test.json\",\"test.json\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n##Dataloader\n<br>\n", "class Dataloader():<br>\n", "  def __init__(self, dataset, trainSize: float):<br>\n", "    self.dataset=dataset<br>\n", "    self.trainSize=trainSize<br>\n", "  def trainTestDataloader(self):<br>\n", "    trainDataset, testDataset=self.splitDataset(self.dataset)<br>\n", "    trainDataloader=DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True)<br>\n", "    testDataloader=DataLoader(testDataset,batch_size=BATCH_SIZE,shuffle=True)<br>\n", "    return trainDataloader, testDataloader<br>\n", "  def splitDataset(self, dataset):<br>\n", "    trainSize = int(self.trainSize * len(self.dataset))<br>\n", "    testSize = len(self.dataset) - trainSize<br>\n", "    trainDataset, testDataset = torch.utils.data.random_split(dataset, [trainSize, testSize])<br>\n", "    return trainDataset, testDataset<br>\n", "#Model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PosModel(nn.Module):\n", "  def __init__(self, vocabilarySize, hiddenSize, output):\n", "    super().__init__()\n", "    self.squential=nn.Sequential(\n", "      nn.Embedding(vocabilarySize, hiddenSize),\n", "      nn.RNN(hiddenSize, output, 10)\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def forward(self, sentance: torch.tensor):\n", "    return self.squential(sentance)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inputSize=len(dataset.tokinizerSentance.tokinizerDict)\n", "outputSize=len(dataset.tokinizerLabel.tokinizerDict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["posModel=PosModel(inputSize,5,outputSize).to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n#Optimizer and loss\n<br>\n", "def parametersCount(model):<br>\n", "  return sum(p.numel() for p in model.parameters() if p.requires_grad)<br>\n", "print(f\"model parameters: {parametersCount(posModel):,}\")<br>\n", "optimizer=torch.optim.Adam(posModel.parameters(),lr=0.001)<br>\n", "loss=nn.CrossEntropyLoss()<br>\n", "Training\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ModelTrainer():\n", "  def __init__(self, model, optimizer, loss, dataset, epochs, device):\n", "    self.model=model\n", "    self.optimizer=optimizer\n", "    self.loss=loss\n", "    self.epochs=epochs\n", "    self.device=device\n", "    self.trainDataloader, self.testDataloader = Dataloader(dataset,0.8).trainTestDataloader()\n", "    self.startTraining()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def startTraining(self)->None:\n", "    for epoch in tqdm(range(self.epochs)):\n", "      trainGenerator=self.train()\n", "      trainLoss, trainAccuracy=self.unpackGenerator(trainGenerator)\n", "      \n", "      testGenerator=self.test()\n", "      testLoss, testAccuracy=self.unpackGenerator(testGenerator)\n", "      print(f\"epoch {epoch} | train loss: {trainLoss:.2f}, train accuracy: {trainAccuracy:.2f} | test loss: {testLoss:.2f}, test accuracy: {testAccuracy:.2f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def train(self):\n", "    for input, target in self.trainDataloader:\n", "      self.optimizer.zero_grad()\n", "      prediction=self.model(input)\n", "      loss, accuracy=self.getLossAndAccuracy(prediction, target)\n", "      loss.requires_grad = True\n", "      loss.backward()\n", "      self.optimizer.step()\n", "      yield loss, accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def test(self):\n", "    for input, target in self.testDataloader:\n", "      self.optimizer.zero_grad()\n", "      prediction=self.model(input)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["      loss, accuracy=self.getLossAndAccuracy(prediction, target)\n", "      yield loss, accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def unpackGenerator(self, generator):\n", "    generator=next(iter(generator))\n", "    loss, accuracy=generator[0],generator[1]\n", "    return loss, accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def getLossAndAccuracy(self,prediction,target):\n", "    prediction=prediction[0].argmax(2).float()\n", "    target=target.float()\n", "    prediction_loss=self.loss(prediction,target)\n", "    prediction_acc=self.accuracy(prediction,target)\n", "    return prediction_loss,prediction_acc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  def accuracy(self,predictions,targets):\n", "    assert predictions.shape == targets.shape, \"Shapes of predictions and targets must match.\"\n", "    num_correct = (predictions == targets).sum().item()\n", "    total_samples = targets.numel()\n", "    accuracy_value = num_correct / total_samples\n", "    return accuracy_value*100"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ModelTrainer(posModel, optimizer, loss, dataset, 100, device)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}